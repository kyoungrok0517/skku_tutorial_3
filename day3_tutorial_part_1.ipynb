{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is [spaCy](https://spacy.io/)?\n",
    "\n",
    "**spaCy** is a free, open-source library for advanced Natural Language Processing (NLP) in Python.\n",
    "\n",
    "spaCy provides a variety of linguistic annotations to give you **insights in to text's grammatical structure.** This includes the word types, like the parts of speech, and how the words are related to each other.\n",
    "\n",
    "![spacy](figures/spacy_logo.png \"NLTK\")\n",
    "\n",
    "Similar to NLTK, spaCy provides a range of features and capabilities related to linguistic concepts and even more general machine learning functionalities.\n",
    "\n",
    "Here is the list of things spaCy allows you to do (you can also visualize them using spaCy's built-in function, **displaCy**.\n",
    "\n",
    "\n",
    "| NAME\t| DESCRIPTION |\n",
    "| :------------- | :----------: |\n",
    "**Tokenization** | Segmenting text into words, punctuations marks etc.\n",
    "**Part-of-speech (POS) Tagging**\t | Assigning word types to tokens, like verb or noun.\n",
    "**Dependency Parsing**\t | Assigning syntactic dependency labels, describing the relations between individual tokens, like subject or object.\n",
    "**Lemmatization**\t | Assigning the base forms of words. For example, the lemma of “was” is “be”, and the lemma of “rats” is “rat”.\n",
    "**Sentence Boundary Detection (SBD)** | \tFinding and segmenting individual sentences.\n",
    "**Named Entity Recognition (NER)**\t | Labelling named “real-world” objects, like persons, companies or locations.\n",
    "**Entity Linking (EL)** | \tDisambiguating textual entities to unique identifiers in a knowledge base.\n",
    "**Similarity**\t | Comparing words, text spans and documents and how similar they are to each other.\n",
    "**Text Classification**\t | Assigning categories or labels to a whole document, or parts of a document.\n",
    "**Rule-based Matching**\t | Finding sequences of tokens based on their texts and linguistic annotations, similar to regular expressions.\n",
    "**Training**\t | Updating and improving a statistical model’s predictions.\n",
    "**Serialization**\t | Saving objects to files or byte strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T04:12:16.179566Z",
     "start_time": "2021-07-02T04:12:16.175323Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIf you do not have spacy and its english package installed, un-comment the following two lines and make sure you install them.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "If you do not have spacy and its english package installed, un-comment the following two lines and make sure you install them.\n",
    "'''\n",
    "\n",
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1. Word Tokenization\n",
    "This section uses a trained pipline (i.e., `en_core_web_sm`) to create a `Language` object containing all the components and data needed to process text.\n",
    "\n",
    "The `Language` object will enable you to lemmatize, apply POS tags, dependency parse and even figure out the shape of the tokens present in the given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T07:55:57.405116Z",
     "start_time": "2021-07-02T07:55:54.560840Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the `Language` object\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T07:55:57.417926Z",
     "start_time": "2021-07-02T07:55:57.406684Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple     PROPN  nsubj\n",
      "is        AUX    aux\n",
      "looking   VERB   ROOT\n",
      "at        ADP    prep\n",
      "buying    VERB   pcomp\n",
      "U.K.      PROPN  dobj\n",
      "startup   NOUN   advcl\n",
      "for       ADP    prep\n",
      "$         SYM    quantmod\n",
      "1         NUM    compound\n",
      "billion   NUM    pobj\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "for token in doc:\n",
    "    print(\"{:<10}{:<7}{}\".format(token.text, token.pos_, token.dep_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T07:55:57.572377Z",
     "start_time": "2021-07-02T07:55:57.419344Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The       DET    det\n",
      "first     ADJ    amod\n",
      "Tesla     PROPN  compound\n",
      "Model     PROPN  compound\n",
      "S         PROPN  compound\n",
      "Plaid     PROPN  compound\n",
      "models    NOUN   nsubj\n",
      "are       AUX    ccomp\n",
      "about     ADJ    acomp\n",
      "to        PART   aux\n",
      "be        AUX    auxpass\n",
      "delivered VERB   xcomp\n",
      "to        ADP    prep\n",
      "customers NOUN   pobj\n",
      "on        ADP    prep\n",
      "June      PROPN  compound\n",
      "10th      NOUN   pobj\n",
      ",         PUNCT  punct\n",
      "            SPACE  nsubj\n",
      "but       CCONJ  cc\n",
      "CEO       NOUN   compound\n",
      "Elon      PROPN  compound\n",
      "Musk      PROPN  conj\n",
      "has       AUX    aux\n",
      "just      ADV    advmod\n",
      "announced VERB   ROOT\n",
      "that      SCONJ  mark\n",
      "the       DET    det\n",
      "range     NOUN   npadvmod\n",
      "-         PUNCT  punct\n",
      "topping   VERB   amod\n",
      "1,100hp   NOUN   compound\n",
      "Plaid+    PROPN  compound\n",
      "variant   NOUN   nsubjpass\n",
      "will      AUX    aux\n",
      "no        ADV    neg\n",
      "longer    ADV    advmod\n",
      "be        AUX    auxpass\n",
      "offered   VERB   ccomp\n",
      ".         PUNCT  punct\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"The first Tesla Model S Plaid models are about to be delivered to customers on June 10th, \\\n",
    "            but CEO Elon Musk has just announced that the range-topping 1,100hp Plaid+ variant will no longer be offered.\")\n",
    "for token in doc:\n",
    "    print(\"{:<10}{:<7}{}\".format(token.text, token.pos_, token.dep_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `nlp` object on a string of text will return a processed `Doc` object. You can think of `Doc` (denoted as the object name `doc` here) as a container for accessing linguistic annotations.\n",
    "\n",
    "Take a look at these `Doc` attributes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T07:55:57.654013Z",
     "start_time": "2021-07-02T07:55:57.574483Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_', '__bytes__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__ne__', '__new__', '__pyx_vtable__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', '_bulk_merge', '_get_array_attrs', '_py_tokens', '_realloc', '_vector', '_vector_norm', 'cats', 'char_span', 'copy', 'count_by', 'doc', 'ents', 'extend_tensor', 'from_array', 'from_bytes', 'from_dict', 'from_disk', 'from_docs', 'get_extension', 'get_lca_matrix', 'has_annotation', 'has_extension', 'has_unknown_spaces', 'has_vector', 'is_nered', 'is_parsed', 'is_sentenced', 'is_tagged', 'lang', 'lang_', 'mem', 'noun_chunks', 'noun_chunks_iterator', 'remove_extension', 'retokenize', 'sentiment', 'sents', 'set_ents', 'set_extension', 'similarity', 'spans', 'tensor', 'text', 'text_with_ws', 'to_array', 'to_bytes', 'to_dict', 'to_disk', 'to_json', 'to_utf8_array', 'user_data', 'user_hooks', 'user_span_hooks', 'user_token_hooks', 'vector', 'vector_norm', 'vocab']\n"
     ]
    }
   ],
   "source": [
    "print(dir(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "During the above process, spaCy tokenizes the text as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T07:55:57.813962Z",
     "start_time": "2021-07-02T07:55:57.656767Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple  /  is  /  looking  /  at  /  buying  /  U.K.  /  startup  /  for  /  $  /  1  /  billion  /  "
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "for token in doc:\n",
    "    print(token.text, end=\"  /  \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the raw text is split on whitespace characters, similar to `text.split(' ')`. Then, the tokenizer processes the text from left to right. On each substring, it performs two checks:\n",
    "\n",
    "1. Does the substring match a tokenizer exception rule? For example, **“don’t”** does not contain whitespace, but should be split into two tokens, **“do”** and **“n’t”**, while **“U.K.”** should always remain one token.\n",
    "\n",
    "2. Can a prefix, suffix or infix be split off? For example punctuation like commas, periods, hyphens or quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T07:55:57.952432Z",
     "start_time": "2021-07-02T07:55:57.819003Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do  /  n't  /  do  /  that  /  ,  /  John  /  .  /  You  /  're  /  intimidating  /  him  /  .  /  Let  /  's  /  go  /  .  /  "
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Don't do that, John. You're intimidating him. Let's go.\")\n",
    "for token in doc:\n",
    "    print(token.text,end=\"  /  \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-speech (POS) tags and dependencies\n",
    "The trained pipelines and statistical models within spaCy enable it to **make predictions** of which tag or label most likely applies in this context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T07:55:58.060537Z",
     "start_time": "2021-07-02T07:55:57.957996Z"
    }
   },
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T07:56:07.549443Z",
     "start_time": "2021-07-02T07:55:58.067491Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple               Apple          PROPN     NNP       nsubj     Xxxxx     1         False\n",
      "is                  be             AUX       VBZ       aux       xx        1         True\n",
      "looking             look           VERB      VBG       ROOT      xxxx      1         False\n",
      "at                  at             ADP       IN        prep      xx        1         True\n",
      "buying              buy            VERB      VBG       pcomp     xxxx      1         False\n",
      "U.K.                U.K.           PROPN     NNP       dobj      X.X.      0         False\n",
      "startup             startup        NOUN      NN        advcl     xxxx      1         False\n",
      "for                 for            ADP       IN        prep      xxx       1         True\n",
      "$                   $              SYM       $         quantmod  $         0         False\n",
      "1                   1              NUM       CD        compound  d         0         False\n",
      "billion             billion        NUM       CD        pobj      xxxx      1         False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kyoungrok\\anaconda3\\lib\\site-packages\\spacy\\displacy\\__init__.py:97: UserWarning: [W011] It looks like you're calling displacy.serve from within a Jupyter notebook or a similar environment. This likely means you're already running a local web server, so there's no need to make displaCy start another one. Instead, you should be able to replace displacy.serve with displacy.render to show the visualization.\n",
      "  warnings.warn(Warnings.W011)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"7f38ff8d74a34996a0b4d7aa973fed1a-0\" class=\"displacy\" width=\"1975\" height=\"487.0\" direction=\"ltr\" style=\"max-width: none; height: 487.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Apple</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">looking</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">at</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">buying</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">U.K.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">startup</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">for</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">$</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">SYM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">1</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">billion</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7f38ff8d74a34996a0b4d7aa973fed1a-0-0\" stroke-width=\"2px\" d=\"M70,352.0 C70,177.0 390.0,177.0 390.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7f38ff8d74a34996a0b4d7aa973fed1a-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,354.0 L62,342.0 78,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7f38ff8d74a34996a0b4d7aa973fed1a-0-1\" stroke-width=\"2px\" d=\"M245,352.0 C245,264.5 385.0,264.5 385.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7f38ff8d74a34996a0b4d7aa973fed1a-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,354.0 L237,342.0 253,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7f38ff8d74a34996a0b4d7aa973fed1a-0-2\" stroke-width=\"2px\" d=\"M420,352.0 C420,264.5 560.0,264.5 560.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7f38ff8d74a34996a0b4d7aa973fed1a-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M560.0,354.0 L568.0,342.0 552.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7f38ff8d74a34996a0b4d7aa973fed1a-0-3\" stroke-width=\"2px\" d=\"M595,352.0 C595,264.5 735.0,264.5 735.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7f38ff8d74a34996a0b4d7aa973fed1a-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M735.0,354.0 L743.0,342.0 727.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7f38ff8d74a34996a0b4d7aa973fed1a-0-4\" stroke-width=\"2px\" d=\"M770,352.0 C770,264.5 910.0,264.5 910.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7f38ff8d74a34996a0b4d7aa973fed1a-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M910.0,354.0 L918.0,342.0 902.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7f38ff8d74a34996a0b4d7aa973fed1a-0-5\" stroke-width=\"2px\" d=\"M420,352.0 C420,2.0 1100.0,2.0 1100.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7f38ff8d74a34996a0b4d7aa973fed1a-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advcl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1100.0,354.0 L1108.0,342.0 1092.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7f38ff8d74a34996a0b4d7aa973fed1a-0-6\" stroke-width=\"2px\" d=\"M1120,352.0 C1120,264.5 1260.0,264.5 1260.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7f38ff8d74a34996a0b4d7aa973fed1a-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1260.0,354.0 L1268.0,342.0 1252.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7f38ff8d74a34996a0b4d7aa973fed1a-0-7\" stroke-width=\"2px\" d=\"M1470,352.0 C1470,177.0 1790.0,177.0 1790.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7f38ff8d74a34996a0b4d7aa973fed1a-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">quantmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1470,354.0 L1462,342.0 1478,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7f38ff8d74a34996a0b4d7aa973fed1a-0-8\" stroke-width=\"2px\" d=\"M1645,352.0 C1645,264.5 1785.0,264.5 1785.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7f38ff8d74a34996a0b4d7aa973fed1a-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1645,354.0 L1637,342.0 1653,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7f38ff8d74a34996a0b4d7aa973fed1a-0-9\" stroke-width=\"2px\" d=\"M1295,352.0 C1295,89.5 1795.0,89.5 1795.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7f38ff8d74a34996a0b4d7aa973fed1a-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1795.0,354.0 L1803.0,342.0 1787.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>\n",
       "</figure>\n",
       "</body>\n",
       "</html></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'dep' visualizer\n",
      "Serving on http://0.0.0.0:5000 ...\n",
      "\n",
      "Shutting down server on port 5000.\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for token in doc:\n",
    "    print(\"{:<20}{:<15}{:<10}{:<10}{:<10}{:<10}{:<10}{}\".format(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop))\n",
    "\n",
    "# Call `displacy.serve` for dependency visualization\n",
    "displacy.serve(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T07:56:07.564614Z",
     "start_time": "2021-07-02T07:56:07.553807Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do                  do             AUX       VBP       aux       Xx        1         True\n",
      "n't                 n't            PART      RB        neg       x'x       0         True\n",
      "do                  do             VERB      VB        ROOT      xx        1         True\n",
      "that                that           DET       DT        dobj      xxxx      1         True\n",
      ",                   ,              PUNCT     ,         punct     ,         0         False\n",
      "John                John           PROPN     NNP       npadvmod  Xxxx      1         False\n",
      ".                   .              PUNCT     .         punct     .         0         False\n",
      "You                 you            PRON      PRP       nsubj     Xxx       1         True\n",
      "'re                 be             AUX       VBP       aux       'xx       0         True\n",
      "intimidating        intimidate     VERB      VBG       ROOT      xxxx      1         False\n",
      "him                 he             PRON      PRP       dobj      xxx       1         True\n",
      ".                   .              PUNCT     .         punct     .         0         False\n",
      "Let                 let            VERB      VB        ROOT      Xxx       1         False\n",
      "'s                  's             PRON      PRP       nsubj     'x        0         True\n",
      "go                  go             VERB      VB        ccomp     xx        1         True\n",
      ".                   .              PUNCT     .         punct     .         0         False\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Don't do that, John. You're intimidating him. Let's go.\")\n",
    "for token in doc:\n",
    "    print(\"{:<20}{:<15}{:<10}{:<10}{:<10}{:<10}{:<10}{}\".format(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (NER)\n",
    "\n",
    "A named entity is a “real-world object” that’s assigned a name – for example, a person, a country, a product or a book title. spaCy can **recognize various types of named entities in a document, by asking the model for a prediction.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T07:56:13.459121Z",
     "start_time": "2021-07-02T07:56:07.565834Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "displacy.serve(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T07:56:19.417920Z",
     "start_time": "2021-07-02T07:56:15.472283Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"The first Tesla Model S Plaid models are about to be delivered to customers on June 10th, \\\n",
    "            but CEO Elon Musk has just announced that the range-topping 1,100hp Plaid+ variant will no longer be offered.\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "displacy.serve(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Quick Tip) BIO Tagging (a.k.a IOB schema)\n",
    "\n",
    "How does an NER model work?\n",
    "\n",
    "An NER model typically uses the BIO schema, which tags the beginning of a name entity token as **\"B\"** and the intermediate and last tokens as **\"I\"**.\n",
    "\n",
    "![bio](figures/bio_tagging.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T08:45:22.011781Z",
     "start_time": "2021-07-02T08:45:21.993149Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"San Francisco considers banning sidewalk delivery robots\")\n",
    "\n",
    "# document level\n",
    "ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
    "print(ents)\n",
    "\n",
    "# token level\n",
    "for word in doc:\n",
    "    print(\"{:<10} / {:<2} / {} \".format(word.text, word.ent_iob_, word.ent_type_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T08:45:22.214276Z",
     "start_time": "2021-07-02T08:45:22.193195Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"In the U.S., 328 million doses have been given so far. In the last week, an average of 1.12 million doses per day \\\n",
    "            were administered.\")\n",
    "\n",
    "# document level\n",
    "ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
    "print(ents)\n",
    "\n",
    "# token level\n",
    "for word in doc:\n",
    "    print(\"{:<10} / {:<2} / {} \".format(word.text, word.ent_iob_, word.ent_type_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vectors and similarity\n",
    "**Similarity** of two words is determined by comparing two word vectors or **\"word embeddings\"**.\n",
    "\n",
    "> A **word embedding** is a multi-dimensional meaning representations of a word.\n",
    "\n",
    "> Word vectors can be generated using an algorithm like **word2vec**.\n",
    "\n",
    "Here, we'll be using `en_core_web_lg`, since `en_core_web_sm` we've been using so far does not contain the token-level word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T07:56:41.789499Z",
     "start_time": "2021-07-02T07:56:21.285964Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check if a token has a vector assigned, and get the L2-norm, which can be used to normalize vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T08:09:46.675332Z",
     "start_time": "2021-07-02T08:09:43.013649Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "tokens = nlp(\"dog cat banana afskfsd\")\n",
    "\n",
    "for token in tokens:\n",
    "    print(\"{:<10}{:<7}{:.3f}{:<100}\".format(token.text, token.has_vector, token.vector_norm, token.is_oov))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity between Documents, Spans and Words\n",
    "\n",
    "Pipeline packages that come with built-in word vectors make them available as the **`Token.vector`** attribute.\n",
    "\n",
    "**`Doc.vector`** and **`Span.vector`** will default to an average of their token vectors. \n",
    "\n",
    "Using these vectors, you can compare two objects and make a prediction of **how similar they are.** This is very useful in building recommendation systems or flagging duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T08:20:53.519517Z",
     "start_time": "2021-07-02T08:20:53.472489Z"
    }
   },
   "outputs": [],
   "source": [
    "doc1 = nlp(\"I like salty fries and hamburgers.\")\n",
    "doc2 = nlp(\"Fast food tastes very good.\")\n",
    "\n",
    "# Similarity of two documents\n",
    "print(\"{} <-> {} :\\n[ Similarity : {}]\\n\".format(doc1, doc2, doc1.similarity(doc2)))\n",
    "\n",
    "# Similarity of tokens and spans\n",
    "french_fries = doc1[2:4]\n",
    "burgers = doc1[5]\n",
    "print(\"{} <-> {} :\\n[ Similarity : {}]\\n\".format(french_fries, burgers, french_fries.similarity(burgers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T08:21:04.889586Z",
     "start_time": "2021-07-02T08:21:04.871225Z"
    }
   },
   "outputs": [],
   "source": [
    "doc1 = nlp(\"Hyundai is a company that employs cutting-edge technology to build cars.\")\n",
    "doc2 = nlp(\"Tesla is a car manufacturer that builds electric cars.\")\n",
    "\n",
    "print(\"{} <-> {} :\\n[ Similarity : {}]\\n\".format(doc1, doc2, doc1.similarity(doc2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T08:23:44.942563Z",
     "start_time": "2021-07-02T08:23:44.903751Z"
    }
   },
   "outputs": [],
   "source": [
    "doc1 = nlp(\"I love trees.\")\n",
    "doc2 = nlp(\"I hate trees.\")\n",
    "\n",
    "print(\"{} <-> {} :\\n[ Similarity : {}]\\n\".format(doc1, doc2, doc1.similarity(doc2)))\n",
    "\n",
    "word1 = nlp(\"love\")\n",
    "word2 = nlp(\"hate\")\n",
    "print(\"{} <-> {} :\\n[ Similarity : {}]\\n\".format(word1, word2, word1.similarity(word2)))\n",
    "\n",
    "word3 = nlp(\"tree\")\n",
    "word4 = nlp(\"tree\")\n",
    "print(\"{} <-> {} :\\n[ Similarity : {}]\\n\".format(word3, word4, word3.similarity(word4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "1. **spaCy**\n",
    "    - (1.1) Apply POS tagging and extract all the proper nouns (PROPN) and numbers (NUM) from the given text.\n",
    "    - (1.2) Apply NER and extract all the numbers from the given text, along with their type (e.g., MONEY, CARDINAL, etc.)\n",
    "    - (1.3) For the given set of documents, compare: (i) Document-wise similarity, (ii) Span-wise similarity, and (iii) Word-level similarity\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.1) Apply POS tagging and extract all the proper nouns (PROPN) and numbers (NUM) from the given text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Advanced Micro had unveiled the deal in October last year as part of its battle to overtake chipmaking rival Intel Corp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Just like most of big tech, Nvidia topped in early September, hitting roughly \\$588 a share. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Most analysts assumed a net increase of 7 to 10% from the starting price of 712 dollars per share. Nevertheless, the stock went through the roof when it shattered the 800 dollar cap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.2) Apply NER and extract all the numbers from the given text, along with their type (e.g., MONEY, CARDINAL, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Of 263,000 Amazon factory workers, just over 88,000 (33.6%) had received their first shot and about 43,000 (16.3%) had received both doses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. To support vaccination access for the sector, the federal government announced 13 clinics in multiple locations for aged-care staff and that they will be providing supplementary vaccines until the end of December."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The decision reflects a greater understanding of the real, but extremely low, risk of the clotting disorder called thrombosis with thrombocytopenia (TTS) for people aged 50-59, who are now recommended to have the Pfizer vaccine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.3) For the given set of documents, compare: \n",
    "    (i) Document-wise similarity\n",
    "    (ii) Span-wise similarity\n",
    "    (iii) Word-level similarity\n",
    "\n",
    "**Make sure to extract the \"spans\" and \"words\" straight from the document. Do NOT just write the words (e.g., doc = nlp(\"pop rock\")) and compute the similarity. They must be extracted from the document (e.g., doc = nlp(document[14:20]))**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\n",
    "\n",
    "**[ Document 1 ]**\n",
    "\n",
    "AC/DC are an Australian rock band formed in Sydney in 1973 by Scottish-born brothers Malcolm and Angus Young. Their music has been variously described as hard rock, blues rock, and heavy metal,but the band themselves call it simply \"rock and roll\".\n",
    "\n",
    "**[ Document 2 ]**\n",
    "\n",
    "Queen are a British rock band formed in London in 1970. Their classic line-up was Freddie Mercury (lead vocals, piano), Brian May (guitar, vocals), Roger Taylor (drums, vocals) and John Deacon (bass). Their earliest works were influenced by progressive rock, hard rock and heavy metal, but the band gradually ventured into more conventional and radio-friendly works by incorporating further styles, such as arena rock and pop rock.\n",
    "\n",
    "**[ Spans ]**\n",
    "\n",
    "\"Their music has been variously described as hard rock\" **<->** \"Queen are a British rock band formed in London in 1970\"\n",
    "\n",
    "\"Their music has been variously described as hard rock, blues rock, and heavy metal\" **<->** \"Their earliest works were influenced by progressive rock, hard rock and heavy metal\"\n",
    "\n",
    "**[ Words ]**\n",
    "\n",
    "\"pop rock\" **<->** \"heavy metal\"\n",
    "\n",
    "\"pop rock\" **<->** \"hard rock\"\n",
    "\n",
    "\"Austrailian\" **<->** \"British\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-02T09:19:54.770699Z",
     "start_time": "2021-07-02T09:19:54.762624Z"
    }
   },
   "source": [
    "2.\n",
    "\n",
    "**[ Document 1 ]**\n",
    "\n",
    "Tesla, Inc. is an American electric vehicle and clean energy company based in Palo Alto, California. Tesla's current products include electric cars, battery energy storage from home to grid-scale, solar panels and solar roof tiles, as well as other related products and services. In 2020, Tesla had the highest sales in the plug-in and battery electric passenger car segments, capturing 16% of the plug-in market (which includes plug-in hybrids) and 23% of the battery-electric (purely electric) market. Through its subsidiary Tesla Energy, the company develops and is a major installer of solar photovoltaic energy generation systems in the United States. Tesla Energy is also one of the largest global suppliers of battery energy storage systems, with 3 GWh of battery storage supplied in 2020.\n",
    "\n",
    "**[ Document 2 ]**\n",
    "\n",
    "Apple Inc. is an American multinational technology company that specializes in consumer electronics, computer software, and online services. Apple is the world's largest technology company by revenue (totalling $274.5 billion in 2020) and, since January 2021, the world's most valuable company. As of 2021, Apple is the world's fourth-largest PC vendor by unit sales,[9] and fourth-largest smartphone manufacturer. It is one of the Big Five American information technology companies, along with Amazon, Google, Microsoft, and Facebook\n",
    "\n",
    "**[ Spans ]**\n",
    "\n",
    "\"American electric vehicle and clean energy company based in Palo Alto, California\" **<->** \"American multinational technology company\"\n",
    "\n",
    "\"Tesla's current products include electric cars, battery energy storage from home to grid-scale\" **<->** \"Apple is the world's largest technology company by revenue\"\n",
    "\n",
    "**[ Words ]**\n",
    "\n",
    "\"solar panels\" **<->** \"smartphone\"\n",
    "\n",
    "\"highest sales\" **<->** \"multinational\"\n",
    "\n",
    "\"United States\" **<->** \"Amazon\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
