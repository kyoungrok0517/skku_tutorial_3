{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using word2vec\n",
    "In this tutorial competition, we dig a little \"deeper\" into sentiment analysis. Google's Word2Vec is a deep-learning inspired method that focuses on the meaning of words. Word2Vec attempts to understand meaning and semantic relationships among words. It works in a way that is similar to deep approaches, such as recurrent neural nets or deep neural nets, but is computationally more efficient. This tutorial focuses on Word2Vec for sentiment analysis.\n",
    "\n",
    "### Reference\n",
    "* https://www.kaggle.com/c/word2vec-nlp-tutorial/overview\n",
    "* https://www.kaggle.com/varun08/sentiment-analysis-using-word2vec/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the built-in logging module\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "c4b588ff-c7d5-407a-b4b6-80e8bbc7690c",
    "_uuid": "9112bbd7f06e0dade3180b209ad38ebcf178d909"
   },
   "outputs": [],
   "source": [
    "# Firstly, please note that the performance of google word2vec is better on big datasets.\n",
    "# In this example we are considering only 25000 training examples from the imdb dataset.\n",
    "# Therefore, the performance is similar to the \"bag of words\" model.\n",
    "\n",
    "# Importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# BeautifulSoup is used to remove html tags from the text\n",
    "from bs4 import BeautifulSoup\n",
    "import re  # For regular expressions\n",
    "\n",
    "# Stopwords can be useful to undersand the semantics of the sentence.\n",
    "# Therefore stopwords are not removed while creating the word2vec model.\n",
    "# But they will be removed  while averaging feature vectors.\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# word2vec expects a list of lists.\n",
    "# Using punkt tokenizer for better splitting of a paragraph into sentences.\n",
    "\n",
    "import nltk.data\n",
    "\n",
    "\n",
    "tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from files\n",
    "train = pd.read_csv(\n",
    "    \"./data/labeledTrainData.tsv.gz\",\n",
    "    delimiter=\"\\t\",\n",
    ")\n",
    "test = pd.read_csv(\"./data/testData.tsv.gz\", delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function converts a text to a sequence of words.\n",
    "def review_wordlist(review, remove_stopwords=False):\n",
    "    # 1. Removing html tags\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "\n",
    "    # 2. Removing non-letter.\n",
    "    review_text = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "\n",
    "    # 3. Converting to lower case and splitting\n",
    "    words = review_text.lower().split()\n",
    "\n",
    "    # 4. Optionally remove stopwords\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "\n",
    "    return words\n",
    "\n",
    "\n",
    "# This function splits a review into sentences\n",
    "def review_sentences(review, tokenizer, remove_stopwords=False):\n",
    "    # 1. Using nltk tokenizer\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = []\n",
    "\n",
    "    # 2. Loop for each sentence\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(review_wordlist(raw_sentence, remove_stopwords))\n",
    "\n",
    "    # This returns the list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.<br /><br />Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.<br /><br />The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci's character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ's music.<br /><br />Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.<br /><br />Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ's bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i've gave this subject....hmmm well i don't know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\n"
     ]
    }
   ],
   "source": [
    "text = train[\"review\"].iloc[0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['with',\n",
       "  'all',\n",
       "  'this',\n",
       "  'stuff',\n",
       "  'going',\n",
       "  'down',\n",
       "  'at',\n",
       "  'the',\n",
       "  'moment',\n",
       "  'with',\n",
       "  'mj',\n",
       "  'i',\n",
       "  've',\n",
       "  'started',\n",
       "  'listening',\n",
       "  'to',\n",
       "  'his',\n",
       "  'music',\n",
       "  'watching',\n",
       "  'the',\n",
       "  'odd',\n",
       "  'documentary',\n",
       "  'here',\n",
       "  'and',\n",
       "  'there',\n",
       "  'watched',\n",
       "  'the',\n",
       "  'wiz',\n",
       "  'and',\n",
       "  'watched',\n",
       "  'moonwalker',\n",
       "  'again'],\n",
       " ['maybe',\n",
       "  'i',\n",
       "  'just',\n",
       "  'want',\n",
       "  'to',\n",
       "  'get',\n",
       "  'a',\n",
       "  'certain',\n",
       "  'insight',\n",
       "  'into',\n",
       "  'this',\n",
       "  'guy',\n",
       "  'who',\n",
       "  'i',\n",
       "  'thought',\n",
       "  'was',\n",
       "  'really',\n",
       "  'cool',\n",
       "  'in',\n",
       "  'the',\n",
       "  'eighties',\n",
       "  'just',\n",
       "  'to',\n",
       "  'maybe',\n",
       "  'make',\n",
       "  'up',\n",
       "  'my',\n",
       "  'mind',\n",
       "  'whether',\n",
       "  'he',\n",
       "  'is',\n",
       "  'guilty',\n",
       "  'or',\n",
       "  'innocent'],\n",
       " ['moonwalker',\n",
       "  'is',\n",
       "  'part',\n",
       "  'biography',\n",
       "  'part',\n",
       "  'feature',\n",
       "  'film',\n",
       "  'which',\n",
       "  'i',\n",
       "  'remember',\n",
       "  'going',\n",
       "  'to',\n",
       "  'see',\n",
       "  'at',\n",
       "  'the',\n",
       "  'cinema',\n",
       "  'when',\n",
       "  'it',\n",
       "  'was',\n",
       "  'originally',\n",
       "  'released'],\n",
       " ['some',\n",
       "  'of',\n",
       "  'it',\n",
       "  'has',\n",
       "  'subtle',\n",
       "  'messages',\n",
       "  'about',\n",
       "  'mj',\n",
       "  's',\n",
       "  'feeling',\n",
       "  'towards',\n",
       "  'the',\n",
       "  'press',\n",
       "  'and',\n",
       "  'also',\n",
       "  'the',\n",
       "  'obvious',\n",
       "  'message',\n",
       "  'of',\n",
       "  'drugs',\n",
       "  'are',\n",
       "  'bad',\n",
       "  'm',\n",
       "  'kay',\n",
       "  'visually',\n",
       "  'impressive',\n",
       "  'but',\n",
       "  'of',\n",
       "  'course',\n",
       "  'this',\n",
       "  'is',\n",
       "  'all',\n",
       "  'about',\n",
       "  'michael',\n",
       "  'jackson',\n",
       "  'so',\n",
       "  'unless',\n",
       "  'you',\n",
       "  'remotely',\n",
       "  'like',\n",
       "  'mj',\n",
       "  'in',\n",
       "  'anyway',\n",
       "  'then',\n",
       "  'you',\n",
       "  'are',\n",
       "  'going',\n",
       "  'to',\n",
       "  'hate',\n",
       "  'this',\n",
       "  'and',\n",
       "  'find',\n",
       "  'it',\n",
       "  'boring'],\n",
       " ['some',\n",
       "  'may',\n",
       "  'call',\n",
       "  'mj',\n",
       "  'an',\n",
       "  'egotist',\n",
       "  'for',\n",
       "  'consenting',\n",
       "  'to',\n",
       "  'the',\n",
       "  'making',\n",
       "  'of',\n",
       "  'this',\n",
       "  'movie',\n",
       "  'but',\n",
       "  'mj',\n",
       "  'and',\n",
       "  'most',\n",
       "  'of',\n",
       "  'his',\n",
       "  'fans',\n",
       "  'would',\n",
       "  'say',\n",
       "  'that',\n",
       "  'he',\n",
       "  'made',\n",
       "  'it',\n",
       "  'for',\n",
       "  'the',\n",
       "  'fans',\n",
       "  'which',\n",
       "  'if',\n",
       "  'true',\n",
       "  'is',\n",
       "  'really',\n",
       "  'nice',\n",
       "  'of',\n",
       "  'him',\n",
       "  'the',\n",
       "  'actual',\n",
       "  'feature',\n",
       "  'film',\n",
       "  'bit',\n",
       "  'when',\n",
       "  'it',\n",
       "  'finally',\n",
       "  'starts',\n",
       "  'is',\n",
       "  'only',\n",
       "  'on',\n",
       "  'for',\n",
       "  'minutes',\n",
       "  'or',\n",
       "  'so',\n",
       "  'excluding',\n",
       "  'the',\n",
       "  'smooth',\n",
       "  'criminal',\n",
       "  'sequence',\n",
       "  'and',\n",
       "  'joe',\n",
       "  'pesci',\n",
       "  'is',\n",
       "  'convincing',\n",
       "  'as',\n",
       "  'a',\n",
       "  'psychopathic',\n",
       "  'all',\n",
       "  'powerful',\n",
       "  'drug',\n",
       "  'lord'],\n",
       " ['why', 'he', 'wants', 'mj', 'dead', 'so', 'bad', 'is', 'beyond', 'me'],\n",
       " ['because', 'mj', 'overheard', 'his', 'plans'],\n",
       " ['nah',\n",
       "  'joe',\n",
       "  'pesci',\n",
       "  's',\n",
       "  'character',\n",
       "  'ranted',\n",
       "  'that',\n",
       "  'he',\n",
       "  'wanted',\n",
       "  'people',\n",
       "  'to',\n",
       "  'know',\n",
       "  'it',\n",
       "  'is',\n",
       "  'he',\n",
       "  'who',\n",
       "  'is',\n",
       "  'supplying',\n",
       "  'drugs',\n",
       "  'etc',\n",
       "  'so',\n",
       "  'i',\n",
       "  'dunno',\n",
       "  'maybe',\n",
       "  'he',\n",
       "  'just',\n",
       "  'hates',\n",
       "  'mj',\n",
       "  's',\n",
       "  'music',\n",
       "  'lots',\n",
       "  'of',\n",
       "  'cool',\n",
       "  'things',\n",
       "  'in',\n",
       "  'this',\n",
       "  'like',\n",
       "  'mj',\n",
       "  'turning',\n",
       "  'into',\n",
       "  'a',\n",
       "  'car',\n",
       "  'and',\n",
       "  'a',\n",
       "  'robot',\n",
       "  'and',\n",
       "  'the',\n",
       "  'whole',\n",
       "  'speed',\n",
       "  'demon',\n",
       "  'sequence'],\n",
       " ['also',\n",
       "  'the',\n",
       "  'director',\n",
       "  'must',\n",
       "  'have',\n",
       "  'had',\n",
       "  'the',\n",
       "  'patience',\n",
       "  'of',\n",
       "  'a',\n",
       "  'saint',\n",
       "  'when',\n",
       "  'it',\n",
       "  'came',\n",
       "  'to',\n",
       "  'filming',\n",
       "  'the',\n",
       "  'kiddy',\n",
       "  'bad',\n",
       "  'sequence',\n",
       "  'as',\n",
       "  'usually',\n",
       "  'directors',\n",
       "  'hate',\n",
       "  'working',\n",
       "  'with',\n",
       "  'one',\n",
       "  'kid',\n",
       "  'let',\n",
       "  'alone',\n",
       "  'a',\n",
       "  'whole',\n",
       "  'bunch',\n",
       "  'of',\n",
       "  'them',\n",
       "  'performing',\n",
       "  'a',\n",
       "  'complex',\n",
       "  'dance',\n",
       "  'scene',\n",
       "  'bottom',\n",
       "  'line',\n",
       "  'this',\n",
       "  'movie',\n",
       "  'is',\n",
       "  'for',\n",
       "  'people',\n",
       "  'who',\n",
       "  'like',\n",
       "  'mj',\n",
       "  'on',\n",
       "  'one',\n",
       "  'level',\n",
       "  'or',\n",
       "  'another',\n",
       "  'which',\n",
       "  'i',\n",
       "  'think',\n",
       "  'is',\n",
       "  'most',\n",
       "  'people'],\n",
       " ['if', 'not', 'then', 'stay', 'away'],\n",
       " ['it',\n",
       "  'does',\n",
       "  'try',\n",
       "  'and',\n",
       "  'give',\n",
       "  'off',\n",
       "  'a',\n",
       "  'wholesome',\n",
       "  'message',\n",
       "  'and',\n",
       "  'ironically',\n",
       "  'mj',\n",
       "  's',\n",
       "  'bestest',\n",
       "  'buddy',\n",
       "  'in',\n",
       "  'this',\n",
       "  'movie',\n",
       "  'is',\n",
       "  'a',\n",
       "  'girl'],\n",
       " ['michael',\n",
       "  'jackson',\n",
       "  'is',\n",
       "  'truly',\n",
       "  'one',\n",
       "  'of',\n",
       "  'the',\n",
       "  'most',\n",
       "  'talented',\n",
       "  'people',\n",
       "  'ever',\n",
       "  'to',\n",
       "  'grace',\n",
       "  'this',\n",
       "  'planet',\n",
       "  'but',\n",
       "  'is',\n",
       "  'he',\n",
       "  'guilty'],\n",
       " ['well',\n",
       "  'with',\n",
       "  'all',\n",
       "  'the',\n",
       "  'attention',\n",
       "  'i',\n",
       "  've',\n",
       "  'gave',\n",
       "  'this',\n",
       "  'subject',\n",
       "  'hmmm',\n",
       "  'well',\n",
       "  'i',\n",
       "  'don',\n",
       "  't',\n",
       "  'know',\n",
       "  'because',\n",
       "  'people',\n",
       "  'can',\n",
       "  'be',\n",
       "  'different',\n",
       "  'behind',\n",
       "  'closed',\n",
       "  'doors',\n",
       "  'i',\n",
       "  'know',\n",
       "  'this',\n",
       "  'for',\n",
       "  'a',\n",
       "  'fact'],\n",
       " ['he',\n",
       "  'is',\n",
       "  'either',\n",
       "  'an',\n",
       "  'extremely',\n",
       "  'nice',\n",
       "  'but',\n",
       "  'stupid',\n",
       "  'guy',\n",
       "  'or',\n",
       "  'one',\n",
       "  'of',\n",
       "  'the',\n",
       "  'most',\n",
       "  'sickest',\n",
       "  'liars'],\n",
       " ['i', 'hope', 'he', 'is', 'not', 'the', 'latter']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_sentences(text, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kyoungrok\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:332: MarkupResemblesLocatorWarning: \".\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kyoungrok\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:332: MarkupResemblesLocatorWarning: \"...\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kyoungrok\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:417: MarkupResemblesLocatorWarning: \"http://www.happierabroad.com\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "sentences = []\n",
    "print(\"Parsing sentences from training set\")\n",
    "for review in train[\"review\"]:\n",
    "    sentences += review_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kyoungrok\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "2021-07-03 02:54:48,818 : INFO : collecting all words and their counts\n",
      "2021-07-03 02:54:48,818 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-07-03 02:54:48,881 : INFO : PROGRESS: at sentence #10000, processed 225664 words, keeping 17775 word types\n",
      "2021-07-03 02:54:48,923 : INFO : PROGRESS: at sentence #20000, processed 451582 words, keeping 24944 word types\n",
      "2021-07-03 02:54:48,964 : INFO : PROGRESS: at sentence #30000, processed 670632 words, keeping 30023 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-03 02:54:49,030 : INFO : PROGRESS: at sentence #40000, processed 896478 words, keeping 34329 word types\n",
      "2021-07-03 02:54:49,073 : INFO : PROGRESS: at sentence #50000, processed 1115469 words, keeping 37741 word types\n",
      "2021-07-03 02:54:49,119 : INFO : PROGRESS: at sentence #60000, processed 1336692 words, keeping 40702 word types\n",
      "2021-07-03 02:54:49,166 : INFO : PROGRESS: at sentence #70000, processed 1559365 words, keeping 43300 word types\n",
      "2021-07-03 02:54:49,209 : INFO : PROGRESS: at sentence #80000, processed 1778623 words, keeping 45699 word types\n",
      "2021-07-03 02:54:49,252 : INFO : PROGRESS: at sentence #90000, processed 2002603 words, keeping 48113 word types\n",
      "2021-07-03 02:54:49,294 : INFO : PROGRESS: at sentence #100000, processed 2224101 words, keeping 50180 word types\n",
      "2021-07-03 02:54:49,338 : INFO : PROGRESS: at sentence #110000, processed 2442894 words, keeping 52050 word types\n",
      "2021-07-03 02:54:49,409 : INFO : PROGRESS: at sentence #120000, processed 2665092 words, keeping 54089 word types\n",
      "2021-07-03 02:54:49,457 : INFO : PROGRESS: at sentence #130000, processed 2890948 words, keeping 55829 word types\n",
      "2021-07-03 02:54:49,499 : INFO : PROGRESS: at sentence #140000, processed 3103390 words, keeping 57318 word types\n",
      "2021-07-03 02:54:49,544 : INFO : PROGRESS: at sentence #150000, processed 3328823 words, keeping 59031 word types\n",
      "2021-07-03 02:54:49,608 : INFO : PROGRESS: at sentence #160000, processed 3550557 words, keeping 60569 word types\n",
      "2021-07-03 02:54:49,650 : INFO : PROGRESS: at sentence #170000, processed 3773286 words, keeping 62032 word types\n",
      "2021-07-03 02:54:49,692 : INFO : PROGRESS: at sentence #180000, processed 3994239 words, keeping 63473 word types\n",
      "2021-07-03 02:54:49,734 : INFO : PROGRESS: at sentence #190000, processed 4219119 words, keeping 64766 word types\n",
      "2021-07-03 02:54:49,773 : INFO : PROGRESS: at sentence #200000, processed 4443600 words, keeping 66056 word types\n",
      "2021-07-03 02:54:49,813 : INFO : PROGRESS: at sentence #210000, processed 4663901 words, keeping 67353 word types\n",
      "2021-07-03 02:54:49,853 : INFO : PROGRESS: at sentence #220000, processed 4888988 words, keeping 68632 word types\n",
      "2021-07-03 02:54:49,924 : INFO : PROGRESS: at sentence #230000, processed 5111142 words, keeping 69909 word types\n",
      "2021-07-03 02:54:49,992 : INFO : PROGRESS: at sentence #240000, processed 5338157 words, keeping 71133 word types\n",
      "2021-07-03 02:54:50,032 : INFO : PROGRESS: at sentence #250000, processed 5552543 words, keeping 72322 word types\n",
      "2021-07-03 02:54:50,074 : INFO : PROGRESS: at sentence #260000, processed 5772532 words, keeping 73454 word types\n",
      "2021-07-03 02:54:50,105 : INFO : collected 74218 word types from a corpus of 5920724 raw words and 266885 sentences\n",
      "2021-07-03 02:54:50,106 : INFO : Creating a fresh vocabulary\n",
      "2021-07-03 02:54:50,180 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 retains 19867 unique words (26.768438923172276%% of original 74218, drops 54351)', 'datetime': '2021-07-03T02:54:50.179536', 'gensim': '4.0.1', 'python': '3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'prepare_vocab'}\n",
      "2021-07-03 02:54:50,181 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 leaves 5786829 word corpus (97.73853670598393%% of original 5920724, drops 133895)', 'datetime': '2021-07-03T02:54:50.181536', 'gensim': '4.0.1', 'python': '3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'prepare_vocab'}\n",
      "2021-07-03 02:54:50,259 : INFO : deleting the raw counts dictionary of 74218 items\n",
      "2021-07-03 02:54:50,261 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2021-07-03 02:54:50,263 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 4297971.807254555 word corpus (74.3%% of prior 5786829)', 'datetime': '2021-07-03T02:54:50.263555', 'gensim': '4.0.1', 'python': '3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'prepare_vocab'}\n",
      "2021-07-03 02:54:50,400 : INFO : estimated required memory for 19867 words and 300 dimensions: 57614300 bytes\n",
      "2021-07-03 02:54:50,401 : INFO : resetting layer weights\n",
      "2021-07-03 02:54:50,425 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2021-07-03T02:54:50.425591', 'gensim': '4.0.1', 'python': '3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'build_vocab'}\n",
      "2021-07-03 02:54:50,426 : INFO : Word2Vec lifecycle event {'msg': 'training model with 16 workers on 19867 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10', 'datetime': '2021-07-03T02:54:50.426591', 'gensim': '4.0.1', 'python': '3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'train'}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-b440c60f6d22>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Training model....\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m model = word2vec.Word2Vec(\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab)\u001b[0m\n\u001b[0;32m    417\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_corpus_sanity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 419\u001b[1;33m             self.train(\n\u001b[0m\u001b[0;32m    420\u001b[0m                 \u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m                 \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1060\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1061\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcorpus_iterable\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1062\u001b[1;33m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(\n\u001b[0m\u001b[0;32m   1063\u001b[0m                     \u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1064\u001b[0m                     \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[1;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay, callbacks)\u001b[0m\n\u001b[0;32m   1421\u001b[0m             \u001b[0mthread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1422\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1423\u001b[1;33m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001b[0m\u001b[0;32m   1424\u001b[0m             \u001b[0mprogress_queue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjob_queue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1425\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_corpus_file_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[1;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[0;32m   1276\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1277\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1278\u001b[1;33m             \u001b[0mreport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# blocks if workers too slow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1279\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# a thread reporting that it finished\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1280\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\queue.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "\n",
    "# Creating the model and setting values for the various parameters\n",
    "num_features = 300  # Word vector dimensionality\n",
    "min_word_count = 10  # Minimum word count\n",
    "num_workers = multiprocessing.cpu_count() / 2  # Number of parallel threads\n",
    "context = 10  # Context window size\n",
    "downsampling = 1e-3  # (0.001) Downsample setting for frequent words\n",
    "\n",
    "# Initializing the train model\n",
    "from gensim.models import word2vec\n",
    "\n",
    "print(\"Training model....\")\n",
    "model = word2vec.Word2Vec(\n",
    "    sentences,\n",
    "    workers=num_workers,\n",
    "    vector_size=num_features,\n",
    "    min_count=min_word_count,\n",
    "    window=context,\n",
    "    sample=downsampling,\n",
    ")\n",
    "\n",
    "# Saving the model for later use. Can be loaded using Word2Vec.load()\n",
    "model_name = \"word2vec.model\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few tests: This will print the odd word among them\n",
    "model.wv.doesnt_match(\"man woman dog child kitchen\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.doesnt_match(\"france england germany berlin\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will print the most similar words present in the model\n",
    "model.wv.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(\"awful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model trained on bigger corpus (for better result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/RaRe-Technologies/gensim-data#models\n",
    "import gensim.downloader as api\n",
    "\n",
    "model = api.load(\"glove-wiki-gigaword-100\")  # trained with 6B tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving Word Analogies!\n",
    "\n",
    "* Man is to Woman what King is to ___?\n",
    "* USA is to hamburger what UK is to ___?\n",
    "* Korea is to kimchi what USA is to ___?\n",
    "\n",
    "![](./figures/analogy.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"])\n",
    "# model.most_similar(positive=[\"hamburger\", \"uk\"], negative=[\"usa\"])\n",
    "# model.most_similar(positive=[\"kimchi\", \"usa\"], negative=[\"korea\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will give the total number of words in the vocabolary created from this dataset\n",
    "model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to average all word vectors in a paragraph\n",
    "def featureVecMethod(words, model, num_features):\n",
    "    # Pre-initialising empty numpy array for speed\n",
    "    featureVec = np.zeros(num_features, dtype=\"float32\")\n",
    "    nwords = 0\n",
    "\n",
    "    # Converting Index2Word which is a list to a set for better speed in the execution.\n",
    "    index2word_set = set(model.wv.index_to_key)\n",
    "\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec, model.wv.get_vector(word))\n",
    "\n",
    "    # Dividing the result by number of words to get average\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "# Function for calculating the average feature vector\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews), num_features), dtype=\"float32\")\n",
    "    for review in reviews:\n",
    "        # Printing a statuse1 message every 1000th review\n",
    "        if counter % 1000 == 0:\n",
    "            print(\"Review %d of %d\" % (counter, len(reviews)))\n",
    "\n",
    "        reviewFeatureVecs[counter] = featureVecMethod(review, model, num_features)\n",
    "        counter = counter + 1\n",
    "\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculating average feature vector for training set\n",
    "clean_train_reviews = []\n",
    "for review in train[\"review\"]:\n",
    "    cleaned = review_wordlist(review, remove_stopwords=True)\n",
    "    clean_train_reviews.append(cleaned)\n",
    "\n",
    "trainDataVecs = getAvgFeatureVecs(clean_train_reviews, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculating average feature vactors for test set\n",
    "clean_test_reviews = []\n",
    "for review in test[\"review\"]:\n",
    "    cleaned = review_wordlist(review, remove_stopwords=True)\n",
    "    clean_test_reviews.append(cleaned)\n",
    "\n",
    "testDataVecs = getAvgFeatureVecs(clean_test_reviews, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a random forest classifier to the training data\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "print(\"Fitting random forest to training data....\")\n",
    "forest = forest.fit(trainDataVecs, train[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the sentiment values for test data and saving the results in a csv file\n",
    "result = forest.predict(testDataVecs)\n",
    "output = pd.DataFrame(data={\"id\": test[\"id\"], \"sentiment\": result})\n",
    "output.to_csv(\"output.csv\", index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit the output at https://www.kaggle.com/c/word2vec-nlp-tutorial/leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: Aspect-base Sentiment Analysis \n",
    "* ref: https://towardsdatascience.com/aspect-based-sentiment-analysis-using-spacy-textblob-4c8de3e0d2b9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from pprint import pprint\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"The food we had yesterday was delicious\",\n",
    "    \"My time in Italy was very enjoyable\",\n",
    "    \"I found the meal to be tasty\",\n",
    "    \"The internet was slow.\",\n",
    "    \"Our experience was suboptimal\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, we pick up the sentiment description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The food we had yesterday was delicious\n",
      "delicious\n",
      "\n",
      "My time in Italy was very enjoyable\n",
      "enjoyable\n",
      "\n",
      "I found the meal to be tasty\n",
      "tasty\n",
      "\n",
      "The internet was slow.\n",
      "slow\n",
      "\n",
      "Our experience was suboptimal\n",
      "suboptimal\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    doc = nlp(sentence)\n",
    "    descriptive_term = \"\"\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"ADJ\":\n",
    "            descriptive_term = token\n",
    "    print(sentence)\n",
    "    print(descriptive_term)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to also extract intensifiers (e.g., \"very\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The food we had yesterday was delicious\n",
      "delicious\n",
      "\n",
      "My time in Italy was very enjoyable\n",
      "very enjoyable\n",
      "\n",
      "I found the meal to be tasty\n",
      "tasty\n",
      "\n",
      "The internet was slow.\n",
      "slow\n",
      "\n",
      "Our experience was suboptimal\n",
      "suboptimal\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    doc = nlp(sentence)\n",
    "    descriptive_term = \"\"\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"ADJ\":\n",
    "            prepend = \"\"\n",
    "            for child in token.children:\n",
    "                if child.pos_ != \"ADV\":\n",
    "                    continue\n",
    "                prepend += child.text + \" \"\n",
    "            descriptive_term = prepend + token.text\n",
    "    print(sentence)\n",
    "    print(descriptive_term)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, identify the targets of the sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"5b9ff9f2cbcc4e92983b649bc933827b-0\" class=\"displacy\" width=\"1275\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">The</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">food</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">we</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">had</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">yesterday</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">was</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">delicious</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5b9ff9f2cbcc4e92983b649bc933827b-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,177.0 215.0,177.0 215.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5b9ff9f2cbcc4e92983b649bc933827b-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5b9ff9f2cbcc4e92983b649bc933827b-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,2.0 925.0,2.0 925.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5b9ff9f2cbcc4e92983b649bc933827b-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5b9ff9f2cbcc4e92983b649bc933827b-0-2\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5b9ff9f2cbcc4e92983b649bc933827b-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,266.5 L412,254.5 428,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5b9ff9f2cbcc4e92983b649bc933827b-0-3\" stroke-width=\"2px\" d=\"M245,264.5 C245,89.5 570.0,89.5 570.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5b9ff9f2cbcc4e92983b649bc933827b-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">relcl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M570.0,266.5 L578.0,254.5 562.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5b9ff9f2cbcc4e92983b649bc933827b-0-4\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5b9ff9f2cbcc4e92983b649bc933827b-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">npadvmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M740.0,266.5 L748.0,254.5 732.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5b9ff9f2cbcc4e92983b649bc933827b-0-5\" stroke-width=\"2px\" d=\"M945,264.5 C945,177.0 1090.0,177.0 1090.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5b9ff9f2cbcc4e92983b649bc933827b-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1090.0,266.5 L1098.0,254.5 1082.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(sentences[0])\n",
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'aspect': 'food', 'description': 'delicious'},\n",
      " {'aspect': 'time', 'description': 'very enjoyable'},\n",
      " {'aspect': 'meal', 'description': 'tasty'},\n",
      " {'aspect': 'internet', 'description': 'slow'},\n",
      " {'aspect': 'experience', 'description': 'suboptimal'}]\n"
     ]
    }
   ],
   "source": [
    "aspects = []\n",
    "for sentence in sentences:\n",
    "    doc = nlp(sentence)\n",
    "    descriptive_term = \"\"\n",
    "    target = \"\"\n",
    "    for token in doc:\n",
    "        if token.dep_ == \"nsubj\" and token.pos_ == \"NOUN\":\n",
    "            target = token.text\n",
    "        if token.pos_ == \"ADJ\":\n",
    "            prepend = \"\"\n",
    "            for child in token.children:\n",
    "                if child.pos_ != \"ADV\":\n",
    "                    continue\n",
    "                prepend += child.text + \" \"\n",
    "            descriptive_term = prepend + token.text\n",
    "    aspects.append({\"aspect\": target, \"description\": descriptive_term})\n",
    "pprint(aspects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify the sentiment using `TextBlob`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'aspect': 'food',\n",
      "  'description': 'delicious',\n",
      "  'sentiment': Sentiment(polarity=1.0, subjectivity=1.0)},\n",
      " {'aspect': 'time',\n",
      "  'description': 'very enjoyable',\n",
      "  'sentiment': Sentiment(polarity=0.65, subjectivity=0.78)},\n",
      " {'aspect': 'meal',\n",
      "  'description': 'tasty',\n",
      "  'sentiment': Sentiment(polarity=0.0, subjectivity=0.0)},\n",
      " {'aspect': 'internet',\n",
      "  'description': 'slow',\n",
      "  'sentiment': Sentiment(polarity=-0.30000000000000004, subjectivity=0.39999999999999997)},\n",
      " {'aspect': 'experience',\n",
      "  'description': 'suboptimal',\n",
      "  'sentiment': Sentiment(polarity=0.0, subjectivity=0.0)}]\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "for aspect in aspects:\n",
    "    aspect[\"sentiment\"] = TextBlob(aspect[\"description\"]).sentiment  # or other sentiment classifiers\n",
    "pprint(aspects)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
